// ============================================================================
//                                Definitions
// ============================================================================
Machine Learning
    Arthur Samuel (1959) - Field of study that gives computers the ability to learn without being explicitly programmed.
    Tom Mitchell (1998) A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.

// ============================================================================
//                                 Notation
// ============================================================================
m = Number of training examples
x = input variable / features
y = output variable / target variable
(x, y) = one training example
(x^(i), y^(i)) = ith training example
h = hypothesis function (output by the learning algorithm)
        h(x) = y

:= will represent the assignment operator, e.g., a := a+1
= will represent a truth assertion, (a = b) ? "equal" : "not equal"

// ============================================================================
//                    Linear Regression With One Variable
// ============================================================================
Also called "Univariate Linear Regression"

The squared error cost function J(theta0, theta1) is generally a good cost function to minimize

Gradient Descent Algorithm
    Used to minimize the multivariable cost function J(O1, O2) by taking repeated, simultaneous steps according to:
    O1 := O1 - alpha(d/dO1) J(O1)
    O2 := O2 - alpha(d/dO2) J(O2)

    until the values converge

    This algorithm is susceptible to falling into relative optima, but linear regression cost functions
    are convex (bowl shaped), so gradient descent will always converge to the global minimum.

    "Batch" Gradient Descent
        This references the fact that the gradient descent algorithm specified here looks at the
        entire training set and sums the error over ALL training data points, at each step

    There's a technique "normal equation method" which solves numerically for the minimum, but it doesn't
    scale as well as gradient descent

    We use linear algebra matrices and notation to handle cases where there are many more features

// ============================================================================
//                 Linear Regression With Multiple Variables
// ============================================================================
Feature Scaling:
    We need to make sure that each of our n features is on a similar scale, otherwise gradient descent may take a very long time
    to find the global minimum, because the elipses in the graph are extremely elongated

    We could scale all of the features so that they're similar--say, in [-1, 1]--then gradient descent will converge
    must faster.

    We generally don't want it to be too much bigger or smaller--as a rule of thumb, anything bigger than
    -3 to 3 or smaller than -1/3 to 1/3, then it probably needs to be scaled first.

    Mean Normalization: Can make it so that the mean has a value 0 in normalization, by applying: x(i) = (x(i) - mean) / (max value - min value)

Learning Rate:
    When alpha is chosen properly, J(theta) should be monotonically decreasing when plotted against iterations
    If it's ever increasing, use a smaller alpha

    Plot J(theta) with value on the y-axis, and number of gradient descent iterations on the x-axis

    We can also look at the plots to determine if gradient descent has converged or not. This tends to be easiest in practice, rather
    than trying to use automatic convergence threshold tests (e.g., if iter(n+1) returns a value that is smaller than iter(n) by less
    than some number epsilon

    If the learning rate seems too slow, try increasing it by ~3x (rule of thumb)

Normal Equation Method:
    This is an alternative to the gradient descent approach. Gradient descent requires choosing alpha, and needs many iterations.
    The normal equation method has neither of these drawbacks.

    However, gradient descent works well even when n (the number of features) is very large, while normal equations are very slow.
        Computing pinv(X'X) gets very expensive: invert runtime is roughly O(n^3)
        As a rule of thumb: if over 10,000 features, you'll probably have to abandon normal equations

    Note that sometimes X'X will not be invertible. Common causes of this are:
        1) Redundant features (linear dependence between features)
        2) Too many features (m <= n)
            This can be solved with a technique called regularization

// ============================================================================
//                               Vectorization
// ============================================================================
Implementing things as vector multiplication instead of for-loops can be
vastly faster

// ============================================================================
//                            Logistic Regression
// ============================================================================
In classification, we may denote "0" as the "Negative Class" and 1 as the "Positive Class"
Generally, this type of problem doesn't mesh too well with linear regression

Logistic Regression creates an h_theta(x) hypothesis that is always between 0 and 1
    by using the Sigmoid function (also known as the Logistic function)

Because the sigmoid function is more complex, our standard squared error cost function
    will result in a J(theta) that is non-convex, so it won't work for gradient descent
    Instead, we use the piecewise defined cost function using logs

Feature scaling also applies to logistic regression, as it did for linear regression

Advanced Optimizations:
    Instead of Gradient descent, we could use conjugate gradient, BFGS, or L-BFGS
    These are more compex and sophisticated, with the following advantages:
        * No need to manually pick alpha
        * Often faster than gradient descent
    They only need the same inputs as Gradient descent: A cost function J(theta), and
    the derivative terms d/dtheta j(J(theta))

    Octave has a good library implementing these algorithm

// ============================================================================
//                              Neural Networks
// ============================================================================
h_theta(x) is a sigmoid (logistic) activation function that takes input vectors x and theta
    and generates some output

We have an input layer, and output layer, and "hidden layer(s)" in between

See Model Representation II for a discussion of a vectorized implementation of forward propagation


// ============================================================================
//                             Practical Advice
// ============================================================================
If we have a learning algorithm that is getting unsatisfactory results, we can try:
    * Getting more training examples?
        - Fixes high variance
    * Try fewer features?
        - Fixes high variance
    * Try more features?
        - Fixes high bias
    * Try adding polynomial features?
        - Fixes high bias
    * Try decreasing lambda?
        - Fixes high bias
    * Try increasing lambda?
        - Fixes high variance

Evaluating a hypothesis with diagnostics:
    Try splitting ~70% of the data into a training set, and ~30% of the data into a test set
    Then measure the J(theta)_test cost if it's linear regression, or %-misclassified if it's logistic regression

If we're considering multiple order polynomials and optimizing for that, then split the training data
    into the training set, the cross-validation set, and the test set (~60%, ~20%, ~20%)
    We then train all the hypothesis on the training set,
    then use the cross-validation set to select a model with the lowest cross-validation error,
    and then measure its generalization score with the test set

Bias vs. Variance
    When we're underfitting, we say that the fit has high bias
        Symptom: both training and cross validation error are high
    When overfitting, we say that the fit has high variance
        Symptom: cross validation error is high, but training error is low
    When using regularization, we don't include the regularization term in the cost functions for the training set and CV set

Skewed Classes (have many more examples from one class than from the other)
    Use a different evaluation metric, such as precision/recall

    By convention, set y = 1 as the extremely rare class
    A true positive is when the algorithm predicts true, and the actual result is true

    Precision: Of examples where we predict y = 1, what fraction is actually 1?
        # True Positive / (# True Positive + # False positives)

    Recall:
        # True Positves / (# True Positives + # False Negatives)

    We want both of these fractions to be as high as possible

    This prevents an algorithm from "cheating" by just always classifying as 0 or 1

    Since these metrics are generally inversely proportional, we need to compute an F-score:
        F_1 score = 2 * (P * R) / (P + R)
        This penalizes having a very low P or R value

How much data to train on
    In order for the "big data" approach to work, the features in X must be sufficient to predict y
    See if a human can predict y accurately, given X.

// ============================================================================
//                          Support Vector Machines
// ============================================================================
Large margin classifier: a margin is the boundary on either side of the decision boundary
    SVMs try to separate the positive and negative exames with as big a margine as possible
    Gives the decision boundary some robustness
