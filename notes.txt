// ============================================================================
//                                Definitions
// ============================================================================
Machine Learning
    Arthur Samuel (1959) - Field of study that gives computers the ability to learn without being explicitly programmed.
    Tom Mitchell (1998) A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.

// ============================================================================
//                                 Notation
// ============================================================================
m = Number of training examples
x = input variable / features
y = output variable / target variable
(x, y) = one training example
(x^(i), y^(i)) = ith training example
h = hypothesis function (output by the learning algorithm)
        h(x) = y

:= will represent the assignment operator, e.g., a := a+1
= will represent a truth assertion, (a = b) ? "equal" : "not equal"

// ============================================================================
//                    Linear Regression With One Variable
// ============================================================================
Also called "Univariate Linear Regression"

The squared error cost function J(theta0, theta1) is generally a good cost function to minimize

Gradient Descent Algorithm
    Used to minimize the multivariable cost function J(O1, O2) by taking repeated, simultaneous steps according to:
    O1 := O1 - alpha(d/dO1) J(O1)
    O2 := O2 - alpha(d/dO2) J(O2)

    until the values converge

    This algorithm is susceptible to falling into relative optima, but linear regression cost functions
    are convex (bowl shaped), so gradient descent will always converge to the global minimum.

    "Batch" Gradient Descent
        This references the fact that the gradient descent algorithm specified here looks at the
        entire training set and sums the error over ALL training data points, at each step

    There's a technique "normal equation method" which solves numerically for the minimum, but it doesn't
    scale as well as gradient descent

    We use linear algebra matrices and notation to handle cases where there are many more features

// ============================================================================
//                 Linear Regression With Multiple Variables
// ============================================================================
Feature Scaling:
    We need to make sure that each of our n features is on a similar scale, otherwise gradient descent may take a very long time
    to find the global minimum, because the elipses in the graph are extremely elongated

    We could scale all of the features so that they're similar--say, in [-1, 1]--then gradient descent will converge
    must faster.

    We generally don't want it to be too much bigger or smaller--as a rule of thumb, anything bigger than
    -3 to 3 or smaller than -1/3 to 1/3, then it probably needs to be scaled first.

    Mean Normalization: Can make it so that the mean has a value 0 in normalization, by applying: x(i) = (x(i) - mean) / (max value - min value)

Learning Rate:
    When alpha is chosen properly, J(theta) should be monotonically decreasing when plotted against iterations
    If it's ever increasing, use a smaller alpha

    Plot J(theta) with value on the y-axis, and number of gradient descent iterations on the x-axis

    We can also look at the plots to determine if gradient descent has converged or not. This tends to be easiest in practice, rather
    than trying to use automatic convergence threshold tests (e.g., if iter(n+1) returns a value that is smaller than iter(n) by less
    than some number epsilon

    If the learning rate seems too slow, try increasing it by ~3x (rule of thumb)

Normal Equation Method:
    This is an alternative to the gradient descent approach. Gradient descent requires choosing alpha, and needs many iterations.
    The normal equation method has neither of these drawbacks.

    However, gradient descent works well even when n (the number of features) is very large, while normal equations are very slow.
        Computing pinv(X'X) gets very expensive: invert runtime is roughly O(n^3)
        As a rule of thumb: if over 10,000 features, you'll probably have to abandon normal equations

    Note that sometimes X'X will not be invertible. Common causes of this are:
        1) Redundant features (linear dependence between features)
        2) Too many features (m <= n)
            This can be solved with a technique called regularization

// ============================================================================
//                               Vectorization
// ============================================================================
Implementing things as vector multiplication instead of for-loops can be
vastly faster

// ============================================================================
//                            Logistic Regression
// ============================================================================
In classification, we may denote "0" as the "Negative Class" and 1 as the "Positive Class"
Generally, this type of problem doesn't mesh too well with linear regression

Logistic Regression creates an h_theta(x) hypothesis that is always between 0 and 1
    by using the Sigmoid function (also known as the Logistic function)

Because the sigmoid function is more complex, our standard squared error cost function
    will result in a J(theta) that is non-convex, so it won't work for gradient descent
    Instead, we use the piecewise defined cost function using logs

Feature scaling also applies to logistic regression, as it did for linear regression

Advanced Optimizations:
    Instead of Gradient descent, we could use conjugate gradient, BFGS, or L-BFGS
    These are more compex and sophisticated, with the following advantages:
        * No need to manually pick alpha
        * Often faster than gradient descent
    They only need the same inputs as Gradient descent: A cost function J(theta), and
    the derivative terms d/dtheta j(J(theta))

    Octave has a good library implementing these algorithm

// ============================================================================
//                              Neural Networks
// ============================================================================
h_theta(x) is a sigmoid (logistic) activation function that takes input vectors x and theta
    and generates some output

We have an input layer, and output layer, and "hidden layer(s)" in between

See Model Representation II for a discussion of a vectorized implementation of forward propagation


// ============================================================================
//                             Practical Advice
// ============================================================================
If we have a learning algorithm that is getting unsatisfactory results, we can try:
    * Getting more training examples?
        - Fixes high variance
    * Try fewer features?
        - Fixes high variance
    * Try more features?
        - Fixes high bias
    * Try adding polynomial features?
        - Fixes high bias
    * Try decreasing lambda?
        - Fixes high bias
    * Try increasing lambda?
        - Fixes high variance

Evaluating a hypothesis with diagnostics:
    Try splitting ~70% of the data into a training set, and ~30% of the data into a test set
    Then measure the J(theta)_test cost if it's linear regression, or %-misclassified if it's logistic regression

If we're considering multiple order polynomials and optimizing for that, then split the training data
    into the training set, the cross-validation set, and the test set (~60%, ~20%, ~20%)
    We then train all the hypothesis on the training set,
    then use the cross-validation set to select a model with the lowest cross-validation error,
    and then measure its generalization score with the test set

Bias vs. Variance
    When we're underfitting, we say that the fit has high bias
        Symptom: both training and cross validation error are high
    When overfitting, we say that the fit has high variance
        Symptom: cross validation error is high, but training error is low
    When using regularization, we don't include the regularization term in the cost functions for the training set and CV set

Skewed Classes (have many more examples from one class than from the other)
    Use a different evaluation metric, such as precision/recall

    By convention, set y = 1 as the extremely rare class
    A true positive is when the algorithm predicts true, and the actual result is true

    Precision: Of examples where we predict y = 1, what fraction is actually 1?
        # True Positive / (# True Positive + # False positives)

    Recall:
        # True Positves / (# True Positives + # False Negatives)

    We want both of these fractions to be as high as possible

    This prevents an algorithm from "cheating" by just always classifying as 0 or 1

    Since these metrics are generally inversely proportional, we need to compute an F-score:
        F_1 score = 2 * (P * R) / (P + R)
        This penalizes having a very low P or R value

How much data to train on
    In order for the "big data" approach to work, the features in X must be sufficient to predict y
    See if a human can predict y accurately, given X.

// ============================================================================
//                          Support Vector Machines
// ============================================================================
Large margin classifier: a margin is the boundary on either side of the decision boundary
    SVMs try to separate the positive and negative exames with as big a margine as possible
    Gives the decision boundary some robustness

When choosing C (1/lambda), we optimize between the following:
    Large C --> Lower bias, higher variance
    Small C --> Higher bias, lower variance

When choosing sigma for the gaussian kernel,
    Large sigma --> higher bias, lower variance
    Small sigma --> lower bias, higher variance

liblinear and libsvm are Andrew's two favorite SVM software packages

Things you need to do:
    Choose paramter C
    Choose the kernel (similarity function):
        No kernel ("linear kernel"): predicts y=1 if Theta' * x >= 0
        Gaussian Kernel
            Need to choose sigma
            You may need to provide a function that computes the similarity function:
                function f = kernel(x1, x2)
            Need to do feature scaling first!
        Whatever kernel you choose MUST satisfy Mercer's Theorem
        Other kernels are used much, much more rarely
            Polynomial kernel, String kernel, chi-squared kernel, histogram intersection kernel

// ============================================================================
//                         Comparison of Techniques
// ============================================================================
Logistic regression and an SVM without a kernel will usually give similar results
Neural networks will almost always work well, but may be slower to train
The SVM optimization problem is convex, so it will always find the global minimum

If n is large relative to m, (say, n ~ 10,000, and m is in (10-1000)
   Use logistic regression or an SVM without a kernel
    If you have so many features

If n is small (1-1000) and m is intermediate (10-10,000),
   Use an SVM with a Gaussian Kernel

If n is small (1-1000), and m is large (50,000+)
    Create/add more features, then use logistic regression or SVM without a kernel

// ============================================================================
//                       Principal Component Analysis
// ============================================================================
By far the most common algorithm used for dimensionality reduction

Tries to find a lower dimensional surface such that the sum of projection error
    on all training examples is minimized

Apply feature scaling and mean normalization first!

1) Compute covariance matrix
    Sigma = (1/m) * X' * X;
2) Compute eigenvectors of the covariance matrix
    [U, S, V ] = svd(Sigma);
3) Grab the first k columns:
    Ureduce = U(:, 1:k);
    z = Ureduce' * x;

Choosing k:
    Use the diagonal S matrix from svd(Sigma)!

Practical Advice
    Run PCA on the training set only, which creates some mapping function from x in R^m to z in R^k

    New training examples and x examples from the CV and test sets are then run through
    the same PCA dimensionality reduction.

    When we're using PCA for compression (less memory/disk, faster training), choose k by
    % of variance retained

    When using PCA for visualization, we need to choose k = 2 or k = 3

    A frequent misuse of PCA: DO NOT use this to try to prevent overfitting!
    This might actuall work out OK, but using regularization is much
    better. Intuitively, this is because PCA doesn't make any use of the labels y
    when doing its reduction, while regularization has that information to take into
    account when it makes its restrictive decisions.

    Before using PCA, you should generally try your ML system WITHOUT PCA first
    Only use it if you need it. Only use PCA if that doesn't work due to scalability.

// ============================================================================
//                             Anomaly Detection
// ============================================================================
Intuition: for a new test case x, we will look at the avg and stddev for each feature of x,
    and conclude that the probability of x occurring naturally is equal to the product
    of each individual feature appearing with the observed value

Algorithm:
    1) Choose features x(i) that might be indicative of anomalous examples
    2) Fit parameters u1, u2, ..., un, sigma^2_1, sigma^2_2, ..., sigma^2_n
    3) Given a new example x, compute p(x)
    4) Anomaly if p(x) < e

Generally this works well in unsupervised learning, but if we have some labeled data, then
we can develop and evaluate this by splitting our data into training, CV, and test sets
    Calculate u, sigma on the training set to create the model p(x)
    Fit epsilon to the cross validation set with some evaluation metrics (F-score, etc)
    Test on the test set to see overall performance

    Since we generally have heavily skewed data here (many good examples, rare anomalies),
    we should usually use one of the other evaluation metrics instead of classification error

Choose features that may take on unusually large or small values in the event of an anomaly

// ============================================================================
//                            Recommender Systems
// ============================================================================
Collaborative Filtering algorithm is magic!

The collaborative filtering algorithm is also known as "Low rank matrix factorization"
    See the video by this name in week 9 to get an overview of collaborative filtering vectorization

Once we've learned the feature vectors for each movie i, we can recommend the n most "similar"
movies to a user by finding the n movies closest to i, that is, smallest ||x(i) - x(j)||
